{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T00:12:07.962802Z",
     "start_time": "2024-07-02T00:12:07.958290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importare le librerie necessarie\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T00:12:08.005443Z",
     "start_time": "2024-07-02T00:12:07.965381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Percorsi dei file\n",
    "path = 'speed_notebook/data'\n",
    "transformed_csv = 'speed_notebook/data/transformed_carTelemetry.csv'\n",
    "model_path = 'speed_notebook/data/random_forest_model.pkl'\n",
    "\n",
    "# Verificare la presenza dei file\n",
    "if not os.path.exists(transformed_csv):\n",
    "    raise FileNotFoundError(f\"File not found: {transformed_csv}\")\n",
    "else:\n",
    "    print(f\"File {transformed_csv} is in the directory {path}!\")\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"File not found: {model_path}\")\n",
    "else:\n",
    "        print(f\"File {model_path} is in the directory {path}!\")\n",
    "        \n",
    "print('All necessary files are ready for be used!')\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: speed_notebook/data/transformed_carTelemetry.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Verificare la presenza dei file\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(transformed_csv):\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtransformed_csv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtransformed_csv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is in the directory \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: File not found: speed_notebook/data/transformed_carTelemetry.csv"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "car_telemetry_transformed = pd.read_csv(transformed_csv, header=None)\n",
    "\n",
    "print('CSV Head: ')\n",
    "print(car_telemetry_transformed.head())\n",
    "\n",
    "# Impostare i nomi delle colonne direttamente\n",
    "car_telemetry_transformed.columns = ['m_speed', 'm_throttle', 'm_steer', 'm_brake', 'm_clutch', 'm_gear', 'm_engineRPM', 'm_drs', 'm_revLightsPercent', 'm_revLightsBitValue', 'm_brakesTemperature', 'm_tyresSurfaceTemperature', 'm_tyresInnerTemperature', 'm_engineTemperature', 'm_tyresPressure', 'm_surfaceType']\n",
    "\n",
    "print('\\nColumns: ', car_telemetry_transformed.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convertire tutte le colonne in numeri per evitare errori di tipo\n",
    "car_telemetry_transformed = car_telemetry_transformed.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print('CSV Head after conversion: ')\n",
    "print(car_telemetry_transformed.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rimuovere tutte le righe con valori NaN\n",
    "car_telemetry_transformed = car_telemetry_transformed.dropna()\n",
    "\n",
    "print('CSV Head after removing NaN values: ')\n",
    "print(car_telemetry_transformed.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Separare le feature dalla variabile target\n",
    "X = car_telemetry_transformed.drop('m_speed', axis=1)\n",
    "y = car_telemetry_transformed['m_speed']\n",
    "\n",
    "print(f'X: {X}')\n",
    "print(f'y: {y}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Caricare il modello addestrato\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Funzione per effettuare la cross-validazione e calcolare gli errori\n",
    "def cross_validate_model(X, y, model, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    r2_scores = []\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        r2_scores.append(r2)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "        print(f\"Fold {len(mse_scores)} - Mean Squared Error: {mse}\")\n",
    "        print(f\"Fold {len(mse_scores)} - R-squared: {r2}\")\n",
    "        print(f\"Fold {len(mse_scores)} - Mean Absolute Error: {mae}\")\n",
    "\n",
    "        if len(mse_scores) == 3:\n",
    "            print(\"\\n\\tDetailed analysis for fold 3:\")\n",
    "            print(\"\\t\\tActual vs. Predicted values sample:\")\n",
    "            for actual, pred in zip(y_test[:10], y_pred[:10]):\n",
    "                print(f\"\\t\\t\\tActual: {actual}, Predicted: {pred}\")\n",
    "\n",
    "            outliers = y_test[(y_test - y_pred).abs() > 2 * mae]\n",
    "            print(\"\\n\\t\\tOutliers:\")\n",
    "            for outlier in outliers:\n",
    "                print(f'\\t\\t\\t{outlier}')\n",
    "        \n",
    "        print('---------------------------------------------------')\n",
    "            \n",
    "    print(f\"Mean R-squared score: {np.mean(r2_scores)}\")\n",
    "\n",
    "    return mse_scores, r2_scores, mae_scores\n",
    "\n",
    "# Eseguire la cross-validazione sul modello Random Forest\n",
    "mse_scores, r2_scores, mae_scores = cross_validate_model(X, y, model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Funzione per plottare l'importanza delle feature\n",
    "def plot_feature_importance(model, X):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(model, X)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribuzione dei valori reali vs. predetti\n",
    "def plot_actual_vs_predicted(y, y_pred):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(y, bins=30, alpha=0.5, label='Actual')\n",
    "    plt.hist(y_pred, bins=30, alpha=0.5, label='Predicted')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Distribution of Actual vs. Predicted Values')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_pred, y - y_pred)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title('Residuals vs. Predicted Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Esempio di predizioni per l'uso nei grafici\n",
    "y_pred = model.predict(X)\n",
    "plot_actual_vs_predicted(y, y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
